#nlp/provider

* The publication of the landmark transformer model [BERT](https://arxiv.org/pdf/1810.04805.pdf "https://arxiv.org/pdf/1810.04805.pdf") came in 2018. Created at Google,

* BERT’s big conceptual advance is its bidirectional structure (the B in BERT stands for “bidirectional”). The model “looks in both directions” as it analyzes a given word, ==considering both the words that come before and the words that come after==, rather than working unidirectionally from left to right. This additional context allows for richer, more nuanced language modeling.

BERT remains one of the most important transformer-based models in use, frequently treated as a reference against which newer models are compared