<!doctype html><html><head><title></title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/favicon_hub02d7508a1c89b2b7812eab204efeb9a_4223_42x0_resize_box_3.png><meta property="og:title" content><meta property="og:description" content="#readings/toread
Source : https://www.forbes.com/sites/robtoews/2022/02/13/language-is-the-next-great-frontier-in-ai/?sh=66bb30cf5c50 author: [[Rob Toews]] suggested by: [[Noureddine Haouari]]
Language is the cornerstone of human intelligence. The emergence of language was the most important intellectual development in our species’ history. It is through language that we formulate thoughts and communicate them to one another. Language enables us to reason abstractly, to develop complex ideas about what the world is and could be, and to build on these ideas across generations and geographies."><meta property="og:type" content="article"><meta property="og:url" content="https://mohamedallam1991.github.io/posts/dzwriters/readings/language-is-the-next-great-frontier-in-ai/"><meta property="article:section" content="posts"><meta name=description content><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-H4LBG7NDFZ"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-H4LBG7NDFZ",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/site/main-logo_hu9ad2f25a877e6fef77c7a3dbef5094ad_6881_42x0_resize_box_3.png alt=Logo>
Mohamed's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20></a></div></li></ul></div></div><img src=/images/site/main-logo_hu9ad2f25a877e6fef77c7a3dbef5094ad_6881_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hub02d7508a1c89b2b7812eab204efeb9a_4223_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/introduction/ title=Introduction>Introduction</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/category/>Category</a><ul><li><i class="fas fa-plus-circle"></i><a href=/posts/category/sub-category/>Sub-Category</a><ul><li><a href=/posts/category/sub-category/rich-content/ title="Rich Content">Rich Content</a></li></ul></li></ul></li><li><a href=/posts/markdown-sample/ title="Markdown Sample">Markdown Sample</a></li><li><a href=/posts/shortcodes/ title="Shortcodes Sample">Shortcodes Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/mohamed_hu6273c7e1398d8440bb6e2f6c2d07d965_9427037_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name>Mohamed Allam</h5><p>Monday, January 1, 1</p></div><div class=title><h1></h1></div><div class=taxonomy-terms><ul></ul></div><div class=post-content id=post-content><p>#readings/toread</p><p>Source : <a href="https://www.forbes.com/sites/robtoews/2022/02/13/language-is-the-next-great-frontier-in-ai/?sh=66bb30cf5c50">https://www.forbes.com/sites/robtoews/2022/02/13/language-is-the-next-great-frontier-in-ai/?sh=66bb30cf5c50</a>
author: [[Rob Toews]]
suggested by: [[Noureddine Haouari]]</p><h1 id=language-is-the-cornerstone-of-human-intelligence>Language is the cornerstone of human intelligence.</h1><p>The emergence of language was the most important intellectual development in our species’ history. It is through language that we formulate thoughts and communicate them to one another. Language enables us to reason abstractly, to develop complex ideas about what the world is and could be, and to build on these ideas across generations and geographies. Almost nothing about modern civilization would be possible without language.</p><p>Building machines that can understand language has thus been a central goal of the field of artificial intelligence dating back to its earliest days.</p><p>It has proven maddeningly elusive.</p><p>This is because mastering language is what is known as an <a href=https://en.wikipedia.org/wiki/AI-complete title=https://en.wikipedia.org/wiki/AI-complete>“AI-complete” problem</a>: that is, an AI that can truly understand language the way a human can would by implication be capable of any other human-level intellectual activity. Put simply, to solve language is to solve AI.</p><p>This profound and subtle insight is at the heart of the <a href=https://en.wikipedia.org/wiki/Turing_test title=https://en.wikipedia.org/wiki/Turing_test>“Turing test,”</a> introduced by AI pioneer Alan Turing in <a href=https://phil415.pbworks.com/f/TuringComputing.pdf title=https://phil415.pbworks.com/f/TuringComputing.pdf>a groundbreaking 1950 paper</a>. Though often critiqued or misunderstood, the Turing test captures a <a href=https://www.kurzweilai.net/a-wager-on-the-turing-test-why-i-think-i-will-win title=https://www.kurzweilai.net/a-wager-on-the-turing-test-why-i-think-i-will-win>fundamental reality</a> about language and intelligence; as it approaches its 75th birthday, it remains as relevant as it was when Turing first conceived it.</p><p>MORE FROMFORBES ADVISOR</p><p>Humanity has yet to build a machine intelligence with human-level mastery of language. (In other words, no machine intelligence has yet passed the Turing test.) But over the past few years researchers have achieved startling, game-changing breakthroughs in language AI, also called natural language processing (NLP).</p><p>The technology is now at a critical inflection point, poised to make the leap from academic research to widespread real-world adoption. In the process, broad swaths of the business world and our daily lives will be transformed. Given language’s ubiquity, few areas of technology will have a more far-reaching impact on society in the years ahead.</p><h3 id=transformers-a-once-in-a-generation-breakthrough>Transformers: A Once-In-A-Generation Breakthrough</h3><p>The most powerful way to illustrate the capabilities of today’s cutting-edge language AI is to start with a few concrete examples.</p><p>Today’s AI <a href="https://twitter.com/QasimMunye/status/1278750809094750211?s=20&t=GN6OnHrn97_Myzyf9nCuRw" title="https://twitter.com/QasimMunye/status/1278750809094750211?s=20&t=GN6OnHrn97_Myzyf9nCuRw">can correctly answer</a> complex medical queries—and explain the underlying biological mechanisms at play. It <a href="https://twitter.com/zebulgar/status/1283927560435326976?s=20&t=Z27yBOy0MYNHEV6zjExs1Q" title="https://twitter.com/zebulgar/status/1283927560435326976?s=20&t=Z27yBOy0MYNHEV6zjExs1Q">can craft</a> nuanced memos about how to run effective board meetings. It <a href=https://maraoz.com/2020/07/18/openai-gpt3/ title=https://maraoz.com/2020/07/18/openai-gpt3/>can write</a> articles analyzing its own capabilities and limitations, while convincingly pretending to be a human observer. It <a href=https://www.gwern.net/GPT-3 title=https://www.gwern.net/GPT-3>can produce</a> original, sometimes beautiful, poetry and literature.</p><p>(It is worth taking a few moments to inspect these examples yourself.)</p><p>What is behind these astonishing new AI abilities, which just five years ago would have been inconceivable?</p><p>In short: the invention of the transformer, a new neural network architecture that has unleashed vast new possibilities in AI.</p><p>A group of Google researchers introduced the transformer in late 2017 in a <a href=https://arxiv.org/pdf/1706.03762.pdf title=https://arxiv.org/pdf/1706.03762.pdf>now-classic research paper</a>.</p><p>Before transformers, the state of the art in NLP—for instance, LSTMs and the widely-used <a href=https://arxiv.org/pdf/1409.3215.pdf title=https://arxiv.org/pdf/1409.3215.pdf>Seq2Seq architecture</a>—was based on recurrent neural networks. By definition, recurrent neural networks process data sequentially—that is, one word at a time, in the order that the words appear.</p><p>Transformers’ great innovation is to make language processing <em>parallelized</em>, meaning that all the tokens in a given body of text are analyzed at the same time rather than in sequence. In order to support this parallelization, transformers rely on an AI mechanism known as attention. Attention enables a model to consider the relationships between words, even if they are far apart in a text, and to determine which words and phrases in a passage are most important to “pay attention to.”</p><p>Parallelization also makes transformers vastly more computationally efficient than RNNs, meaning that they can be trained on larger datasets and built with more parameters. One defining characteristic of today’s transformer models is their massive size.</p><p>A flurry of innovation followed in the wake of the original transformer paper as the world’s leading AI researchers built upon this foundational breakthrough.</p><p>The publication of the landmark transformer model <a href=https://arxiv.org/pdf/1810.04805.pdf title=https://arxiv.org/pdf/1810.04805.pdf>BERT</a> came in 2018. Created at Google, BERT’s big conceptual advance is its bidirectional structure (the B in BERT stands for “bidirectional”). The model “looks in both directions” as it analyzes a given word, considering both the words that come before and the words that come after, rather than working unidirectionally from left to right. This additional context allows for richer, more nuanced language modeling.</p><p>BERT remains one of the most important transformer-based models in use, frequently treated as a reference against which newer models are compared. Much subsequent research on transformers—for instance, Facebook’s influential <a href=https://arxiv.org/pdf/1907.11692.pdf title=https://arxiv.org/pdf/1907.11692.pdf>RoBERTa model</a> (2019)—is based on refining BERT.</p><p>Google’s entire search engine today is <a href=https://blog.google/products/search/search-language-understanding-bert/ title=https://blog.google/products/search/search-language-understanding-bert/>powered by BERT</a>, one of the most far-reaching examples of transformers’ real-world impact.</p><p>Another core vein of research in the world of transformers is OpenAI’s family of GPT models. OpenAI published the <a href=https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf title=https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf>original GPT</a> in June 2018, <a href=https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf title=https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf>GPT-2</a> in February 2019, and <a href=https://arxiv.org/pdf/2005.14165.pdf title=https://arxiv.org/pdf/2005.14165.pdf>GPT-3</a> in May 2020. Popular <a href=https://www.eleuther.ai/ title=https://www.eleuther.ai/>open-source versions</a> of these models, like GPT-J and GPT-Neo, have followed.</p><p>As the “G” in their names indicates, the GPT models are generative: they generate original text output in response to the text input they are fed. This is an important distinction between the GPT class of models and the BERT class of models. BERT, unlike GPT, does not generate new text but instead analyzes existing text (think of activities like search, classification, or sentiment analysis).</p><p>GPT’s generative capabilities make these models particularly attention-grabbing, since writing appears to be a creative act and the output can be astonishingly human-like. Text generation is sometimes referred to as “NLP’s party trick.” (All four of the examples linked to above are text generation examples from GPT-3.)</p><p>Perhaps the most noteworthy element of the GPT architecture is its sheer size. OpenAI has been intentional and transparent about its strategy to pursue more advanced language AI capabilities through raw scale above all else: more compute, larger training data corpora, larger models.</p><p>With 1.5 billion parameters, GPT-2 was the largest model ever built at the time of its release. Published less than a year later, GPT-3 was two orders of magnitude larger: a whopping 175 billion parameters. <a href=https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/ title=https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/>Rumors have circulated</a> that GPT-4 will contain on the order of <em>100 trillion parameters</em> (perhaps not coincidentally, roughly equivalent to <a href=https://www.scientificamerican.com/article/100-trillion-connections/ title=https://www.scientificamerican.com/article/100-trillion-connections/>the number of synapses</a> in the human brain). As a point of comparison, the largest BERT model had 340 million parameters.</p><p>As with any machine learning effort today, the performance of these models depends above all on the data on which they are trained.</p><p>Today’s transformer-based models learn language by ingesting essentially the entire internet. BERT was fed all of Wikipedia (along with the digitized texts of thousands of unpublished books). RoBERTa improved upon BERT by training on even larger volumes of text from the internet. GPT-3’s training dataset was larger still, consisting of half a trillion language tokens. Thus, these models’ linguistic outputs and behaviors can ultimately be traced to the statistical patterns in all the text that humans have previously published online.</p><p>The reason such large training datasets are possible is that transformers use self-supervised learning, meaning that they learn from unlabeled data. This is a crucial difference between today’s cutting-edge language AI models and the previous generation of NLP models, which had to be trained with labeled data. Today’s self-supervised models can train on far larger datasets than was ever previously possible: after all, there is more unlabeled text data than labeled text data in the world by many orders of magnitude.</p><p>Some observers point to self-supervised learning, and the vastly larger training datasets that this technique unlocks, as the single most important driver of NLP’s dramatic performance gains in recent years, more so than any other feature of the transformer architecture.</p><h3 id=foundation-models>Foundation Models</h3><p>Training models on massive datasets with millions or billions of parameters requires vast computational resources and engineering know-how. This makes large language models prohibitively costly and difficult to build. GPT-3, for example, required several thousand petaflop/second-days to train—a staggering amount of computational resources.</p><p>Because very few organizations in the world have the resources and talent to build large language models from scratch, almost all cutting-edge NLP models today are adapted from a small handful of base models: e.g., BERT, RoBERTa, GPT-2, BART. Almost without exception, these models come from the world’s largest tech companies: Google, Facebook, OpenAI (which is bankrolled by Microsoft), Nvidia.</p><p>Without anyone quite planning for it, this has resulted in an entirely new paradigm for NLP technology development—one that will have profound implications for the nascent AI economy.</p><p>This paradigm can be thought of in two basic phases: pre-training and fine-tuning.</p><p>In the first phase, a tech giant creates and open-sources a large language model: for instance, Google’s BERT or Facebook’s RoBERTa.</p><p>Unlike in previous generations of NLP, in which models had to be built for individual language tasks, these massive models are not specialized for any particular activity. They have powerful generalized language capabilities across functions and topic areas. Out of the box, they perform well at the full gamut of activities that comprise linguistic competence: language classification, language translation, search, question answering, summarization, text generation, conversation. Each of these activities on its own presents compelling technological and economic opportunities.</p><p>Because they can be adapted to any number of specific end uses, these base models are referred to as “pre-trained.”</p><p>In the second phase, downstream users—young startups, academic researchers, anyone else who wants to build an NLP model—take these pre-trained models and refine them with a small amount of additional training data in order to optimize them for their own specific use case or market. This step is referred to as “fine-tuning.”</p><p>“Today’s pre-trained models are incredibly powerful, and even more importantly, they are publicly available,” said Yinhan Liu, lead author on Facebook’s RoBERTa work and now cofounder/CTO of healthcare NLP startup BirchAI. “For those teams that have the know-how to operationalize transformers, the question becomes: what is the most important or impactful use case to which I can apply this technology?”</p><p>Under this “pre-train then fine-tune” paradigm, the heavy lifting is done upfront with the creation of the pre-trained model. Even after fine-tuning, the end model’s behavior remains largely dictated by the pre-trained model’s parameters.</p><p>This makes these pre-trained models incredibly influential. So influential, in fact, that Stanford University has recently coined a new name for them, “foundation models”, and launched an entire academic program devoted to better understanding them: the <a href=https://crfm.stanford.edu/ title=https://crfm.stanford.edu/>Center for Research on Foundation Models</a> (CRFM). The Stanford team believes that foundation models, and the small group of tech giants that have the resources to produce them, will exert outsize influence on the future behavior of artificial intelligence around the world.</p><p>As the researchers <a href=https://arxiv.org/pdf/2108.07258.pdf title=https://arxiv.org/pdf/2108.07258.pdf>put it</a>: “Foundation models have led to an unprecedented level of homogenization: Almost all state-of-the-art NLP models are now adapted from one of a few foundation models. While this homogenization produces extremely high leverage (any improvements in the foundation models can lead to immediate benefits across all of NLP), it is also a liability; all AI systems might inherit the same problematic biases of a few foundation models.”</p><p>This Stanford effort is drawing attention to a massive looming problem for large language models: social bias.</p><p>The source of social bias in AI models is straightforward to summarize but insidiously difficult to root out. Because large language models (or foundation models, to use the new branding) learn language by ingesting what humans have written online, they inevitably inherit the prejudices, false assumptions and harmful beliefs of their imperfect human progenitors. Just imagine all the fringe subreddits and bigoted blogs that must have been included in GPT-3’s vast training data corpus.</p><p>The problem has been <a href=https://arxiv.org/pdf/2106.13219.pdf title=https://arxiv.org/pdf/2106.13219.pdf>extensively documented</a>: today’s most prominent foundation models all exhibit racist, sexist, xenophobic, and other antisocial tendencies. This issue will only grow more acute as foundation models become increasingly influential in society. Some observers believe that AI bias will eventually become as prominent of an issue for consumers, companies and governments as digital threats like data privacy or cybersecurity that have come before it—threats that were also not fully appreciated at first, because the breakneck pace of technological change outstripped society’s ability to properly adapt to it.</p><p>There is no silver-bullet solution to the challenge of AI bias and toxicity. But as the problem becomes more widely recognized, a number of mitigation strategies are being pursued.</p><p>Last month, OpenAI <a href=https://openai.com/blog/instruction-following/ title=https://openai.com/blog/instruction-following/>announced</a> that it had developed a new version of GPT-3 that is “safer, more helpful, and more aligned” with human values. The company used a technique known as <a href=https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf title=https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf>reinforcement learning from human feedback</a> to fine-tune its models to be less biased and more truthful than the original GPT-3. This new version, named InstructGPT, is now the default language model that OpenAI makes available to customers.</p><p>Historically, Alphabet’s DeepMind has been an outlier among the world’s most advanced AI research organizations for <em>not</em> making language AI a major focus area. This changed at the end of 2021, with DeepMind <a href=https://deepmind.com/blog/article/language-modelling-at-scale title=https://deepmind.com/blog/article/language-modelling-at-scale>announcing</a> a collection of important work on large language models.</p><p>Of the three NLP papers that DeepMind published, one is devoted entirely to the ethical and social risks of language AI. The paper <a href=https://arxiv.org/pdf/2112.04359.pdf title=https://arxiv.org/pdf/2112.04359.pdf>proposes</a> a comprehensive taxonomy of 6 thematic areas and 21 specific risks that language models pose, including discrimination, exclusion, toxicity and misinformation. DeepMind pledged to make these risks a central focus of its NLP research going forward to help ensure that it is pursuing innovation in language AI responsibly.</p><p>The fact that this dimension of language AI research—until recently, treated as an afterthought or ignored altogether by most of the world’s NLP researchers—featured so centrally in DeepMind’s recent foray into language AI may be a signal of the field’s shifting priorities moving forward.</p><p>Increased regulatory focus on the harms of bias and toxicity in AI models will only accelerate this shift. And make no mistake: regulatory action on this front is a matter of when, not if.</p><h3 id=beyond-natural-language>Beyond Natural Language</h3><p>Interestingly, perhaps the most creative use cases for NLP today don’t involve natural language at all. In particular, today’s cutting-edge language AI technology is powering remarkable breakthroughs in two other domains: coding and biology.</p><p>Whether it’s Python, Ruby, or Java, computer programming happens via languages. Just like natural languages like English or Swahili, programming languages are symbolically represented, follow regular rules, and have a robust internal logic. The audience just happens to be software compilers rather than other humans.</p><p>It therefore makes sense that the same powerful new technologies that have given AI incredible fluency in natural language can likewise be applied to programming languages, with similar results.</p><p>Last summer OpenAI <a href=https://arxiv.org/pdf/2107.03374.pdf title=https://arxiv.org/pdf/2107.03374.pdf>announced Codex</a>, a transformer-based model that can write computer code astonishingly well. In parallel, GitHub (which is allied with OpenAI through its parent company Microsoft) launched a productized version of Codex <a href=https://github.blog/2021-06-29-introducing-github-copilot-ai-pair-programmer/ title=https://github.blog/2021-06-29-introducing-github-copilot-ai-pair-programmer/>named Copilot</a>.</p><p>To develop Codex, OpenAI took GPT-3 and fine-tuned it on a massive volume of publicly available written code from GitHub.</p><p>Codex’s design is simple: human users give it a plain-English description of a command or function and Codex turns this description into functioning computer code. A user could input into Codex, for instance, “crop this image circularly” or “animate this image horizontally so that it bounces off the left and right walls”—and Codex can produce a snippet of code to implement those actions. (These exact examples can be examined on <a href=https://openai.com/blog/openai-codex/#spacegame title=https://openai.com/blog/openai-codex/#spacegame>OpenAI’s website</a>.) Codex is most capable in Python, but it is proficient in over a dozen programming languages.</p><p>Then, just two weeks ago, DeepMind further advanced the frontiers of AI coding with its <a href=https://www.deepmind.com/blog/article/Competitive-programming-with-AlphaCode title=https://www.deepmind.com/blog/article/Competitive-programming-with-AlphaCode>publication of AlphaCode</a>.</p><p>AlphaCode is an AI system that can compete at a human level in programming competitions. In these competitions, which attract hundreds of thousands of participants each year, contestants receive a lengthy problem statement in English and must construct a complete computer program that solves it. Example problems include devising strategies for a custom board game or solving an arithmetic-based brain teaser.</p><p>While OpenAI’s Codex can produce short snippets of code in response to concrete descriptions, DeepMind’s AlphaCode goes much further. It begins to approach the full complexity of real-world programming: assessing an abstract problem without a clear solution, devising a structured approach to solving it, and then executing on that approach with up to hundreds of lines of code. AlphaCode almost seems to display that ever-elusive attribute in AI, high-level reasoning.</p><p>As DeepMind’s AlphaCode team <a href=https://www.deepmind.com/blog/article/Competitive-programming-with-AlphaCode title=https://www.deepmind.com/blog/article/Competitive-programming-with-AlphaCode>wrote</a>: “Creating solutions to unforeseen problems is second nature in human intelligence—a result of critical thinking informed by experience. For artificial intelligence to help humanity, our systems need to be able to develop problem-solving capabilities. AlphaCode solves new problems in programming competitions that require a combination of critical thinking, logic, algorithms, coding, and natural language understanding.”</p><p>Another “language” in which today’s cutting-edge NLP has begun to generate remarkable insights is biology, from genomics to proteins.</p><p>Genomics is well-suited to the application of large language models because an individual’s entire genetic endowment is encoded in a simple four-letter alphabet: A (for adenine), C (for cytosine), G (for guanine), and T (for thymine). Every human’s DNA is defined by a string of billions of A’s, C’s, G’s and T’s (known as nucleotides) in a particular order.</p><p>In many respects DNA functions like a language, with its nucleotide sequences exhibiting regular patterns that resemble a kind of vocabulary, grammar, and semantics. What does this language say? It defines much about who we are, from our height to our eye color to our risk of heart disease or substance abuse.</p><p>Large language models are now making rapid progress in deciphering the language of DNA, in particular its “noncoding” regions. These noncoding regions do not contain genes but rather <em>control</em> genes: they regulate how much, when, and where given genes are expressed, giving them a central role in the maintenance of life. Noncoding regions comprise 98% of our total DNA but until now have remained poorly understood.</p><p>A few months ago, DeepMind <a href=https://deepmind.com/blog/article/enformer title=https://deepmind.com/blog/article/enformer>introduced</a> a new transformer-based architecture that can predict gene expression based on DNA sequence with unprecedented accuracy. It does so by considering interactions between genes and noncoding DNA sequences at much greater distances than was ever before possible. A team at Harvard completed <a href=https://towardsdatascience.com/bringing-bert-to-the-field-how-to-predict-gene-expression-from-corn-dna-9287af91fcf8 title=https://towardsdatascience.com/bringing-bert-to-the-field-how-to-predict-gene-expression-from-corn-dna-9287af91fcf8>work</a> along similar lines to better understand gene expression in corn (fittingly naming their model “CornBERT”).</p><p>Another subfield of biology that represents fertile ground for language AI is <a href=https://www.sciencedirect.com/science/article/pii/S2001037021000945#! title=https://www.sciencedirect.com/science/article/pii/S2001037021000945#!>the study of proteins</a>. Proteins are strings of building blocks known as amino acids, linked together in a particular order. There are 20 amino acids in total. Thus, for all their complexity, proteins can be treated as tokenized strings—wherein each amino acid, like each word in a natural language, is a token—and analyzed accordingly.</p><p>As one example, an AI research team from Salesforce <a href=https://blog.salesforceairesearch.com/progen/ title=https://blog.salesforceairesearch.com/progen/>recently built</a> an NLP model that “learns the language of proteins” and can generate plausible protein sequences that don’t exist in nature with prespecified characteristics. The potential applications of this type of controllable protein synthesis are tantalizing.</p><p>These efforts are just the beginning. In the months and years ahead, language AI will make profound contributions to our understanding of how life itself works.</p><h3 id=conclusion>Conclusion</h3><p>Language is at the heart of human intelligence. It therefore is and must be at the heart of our efforts to build artificial intelligence. No sophisticated AI can exist without mastery of language.</p><p>Today, the field of language AI is at an exhilarating inflection point, on the cusp of transforming industries and spawning new multi-billion-dollar companies. At the same time, it is fraught with societal dangers like bias and toxicity that are only now starting to get the attention they deserve.</p><p>This article explored the big-picture developments and trends shaping the world of language AI today. In a <a href="https://www.forbes.com/sites/robtoews/2022/03/27/a-wave-of-billion-dollar-language-ai-startups-is-coming/?sh=1e8aa7302b14" title="https://www.forbes.com/sites/robtoews/2022/03/27/a-wave-of-billion-dollar-language-ai-startups-is-coming/?sh=1e8aa7302b14">followup article</a>, we canvass today’s most exciting NLP startups. A growing group of NLP entrepreneurs is applying cutting-edge language AI in creative ways across sectors and use cases, generating massive economic value and profound industry disruption. Few startup categories hold more promise in the years ahead.</p></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"><strong>Share on:</strong>
<a class="btn btn-sm facebook-btn" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fmohamedallam1991.github.io%2fposts%2fdzwriters%2freadings%2flanguage-is-the-next-great-frontier-in-ai%2f" target=_blank><i class="fab fa-facebook"></i></a>
<a class="btn btn-sm twitter-btn" href="https://twitter.com/share?url=https%3a%2f%2fmohamedallam1991.github.io%2fposts%2fdzwriters%2freadings%2flanguage-is-the-next-great-frontier-in-ai%2f&text=&via=Mohamed%27s%20Blog" target=_blank><i class="fab fa-twitter"></i></a>
<a class="btn btn-sm reddit-btn" href="https://reddit.com/submit?url=https%3a%2f%2fmohamedallam1991.github.io%2fposts%2fdzwriters%2freadings%2flanguage-is-the-next-great-frontier-in-ai%2f&title=" target=_blank><i class="fab fa-reddit"></i></a>
<a class="btn btn-sm linkedin-btn" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fmohamedallam1991.github.io%2fposts%2fdzwriters%2freadings%2flanguage-is-the-next-great-frontier-in-ai%2f&title=" target=_blank><i class="fab fa-linkedin"></i></a>
<a class="btn btn-sm whatsapp-btn" href="https://api.whatsapp.com/send?text= https%3a%2f%2fmohamedallam1991.github.io%2fposts%2fdzwriters%2freadings%2flanguage-is-the-next-great-frontier-in-ai%2f" target=_blank><i class="fab fa-whatsapp"></i></a>
<a class="btn btn-sm email-btn" href="mailto:?subject=&body=https%3a%2f%2fmohamedallam1991.github.io%2fposts%2fdzwriters%2freadings%2flanguage-is-the-next-great-frontier-in-ai%2f" target=_blank><i class="fas fa-envelope-open-text"></i></a></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/hugo-toha/hugo-toha.github.io/edit/main/content/posts/dzwriters/Readings/Language%20Is%20The%20Next%20Great%20Frontier%20In%20AI.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Improve this page</a></div></div><hr><div class="row next-prev-navigator"></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><ul><li><a href=#transformers-a-once-in-a-generation-breakthrough>Transformers: A Once-In-A-Generation Breakthrough</a></li><li><a href=#foundation-models>Foundation Models</a></li><li><a href=#beyond-natural-language>Beyond Natural Language</a></li><li><a href=#conclusion>Conclusion</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://mohamedallam1991.github.io/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://mohamedallam1991.github.io/#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=https://mohamedallam1991.github.io/#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=https://mohamedallam1991.github.io/#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=https://mohamedallam1991.github.io/#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=https://mohamedallam1991.github.io/#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=https://mohamedallam1991.github.io/#accomplishments>Accomplishments</a></li><li class=nav-item><a class=smooth-scroll href=https://mohamedallam1991.github.io/#achievements>Achievements</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:johndoe@example.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>johndoe@example.com</span></a></li><li><a href=https://github.com/MohamedAllam1991 target=_blank rel=noopener><span><i class="fab fa-github"></i></span> <span>MohamedAllam1991</span></a></li><li><a href=https://www.linkedin.com/in/Mohamed%20Allam target=_blank rel=noopener><span><i class="fab fa-linkedin"></i></span> <span>Mohamed Allam</span></a></li><li><span><i class="fas fa-phone-alt"></i></span> <span>+0213791812254</span></li></ul></div><div class="col-md-4 col-sm-12"><p>Stay up to date with email notification</p><form action="https://github.us1.list-manage.com/subscribe/post?u=19de52a4603135aae97163fd8&amp;amp;id=094a24c76e" method=post id=mc-embedded-subscribe-form name=mc-embedded-subscribe-form class=validate target=_blank novalidate><div class=form-group><input type=email class=form-control id=mce-EMAIL name=EMAIL aria-describedby=emailHelp placeholder="Enter email">
<small id=emailHelp class="form-text text-muted">By entering your email address, you agree to receive the newsletter of this website.</small></div><button type=submit class="btn btn-info">Submit</button></form></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2022 Copyright!.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script>
<script type=text/javascript src=/js/popper.min.js></script>
<script type=text/javascript src=/js/bootstrap.min.js></script>
<script type=text/javascript src=/js/navbar.js></script>
<script type=text/javascript src=/js/plyr.js></script>
<script type=text/javascript src=/js/main.js></script>
<script type=text/javascript src=/js/darkreader.js></script>
<script type=text/javascript src=/js/darkmode-darkreader.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script>
<script src=/js/single.js></script>
<script>hljs.initHighlightingOnLoad()</script></body></html>